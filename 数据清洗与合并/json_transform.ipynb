{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ffb137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, StorageContext, Settings\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import PromptTemplate\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.core.llms import ChatMessage\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "import torch\n",
    "import pandas as pd \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 检查 CUDA 是否可用并设置设备\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用 GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # 如果没有 GPU，使用 CPU\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7e111d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# 配置vLLM服务端参数\n",
    "class VLLMConfig:\n",
    "    API_BASE = \"http://localhost:8000/v1\"  # vLLM的默认端点\n",
    "    MODEL_NAME = \"DeepSeek-R1-Distill-Qwen-1___5B\"\n",
    "    API_KEY = \"no-key-required\"  # vLLM默认不需要密钥\n",
    "    TIMEOUT = 60  # 请求超时时间\n",
    "\n",
    "# 初始化LLM（替换原来的HuggingFaceLLM）\n",
    "def init_vllm_llm():\n",
    "    return OpenAILike(\n",
    "        model=VLLMConfig.MODEL_NAME,\n",
    "        api_base=VLLMConfig.API_BASE,\n",
    "        api_key=VLLMConfig.API_KEY,\n",
    "        temperature=0.3,\n",
    "        max_tokens=1024,\n",
    "        timeout=VLLMConfig.TIMEOUT,\n",
    "        is_chat_model=True,  # 适用于对话模型\n",
    "        additional_kwargs={\"stop\": [\"<|im_end|>\"]}  # DeepSeek的特殊停止符\n",
    "    )\n",
    "\n",
    "# 在全局设置中配置\n",
    "Settings.llm = init_vllm_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0890f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/vitol/vscode/firm_perf/firm_perf_code/转JSON/json文件汇总/stk_basic_info.json', 'r', encoding='utf-8') as file:\n",
    "\n",
    "    json_list = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nodes(raw_data: List[Dict]) -> List[TextNode]:\n",
    "    \"\"\"添加ID稳定性保障\"\"\"\n",
    "    nodes = []\n",
    "    for entry in raw_data:\n",
    "        j2str = []\n",
    "        content_str = json.dumps(entry['text'],ensure_ascii=False)\n",
    "\n",
    "        id_e = entry[\"id\"]\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            node = TextNode(\n",
    "                    text=content_str,\n",
    "                    id_=id_e,  # 显式设置稳定ID\n",
    "                    metadata={\n",
    "                        \"公司全称\": entry['text']['公司全称'],\n",
    "                        \"公司简称\": entry['text']['公司简称'],\n",
    "                        \"股票代码\": entry['text']['股票代码'],\n",
    "                        \"行业分类\": entry['text']['申万行业分类'],\n",
    "                    }\n",
    "                )\n",
    "            nodes.append(node)\n",
    "        except Exception as e:\n",
    "            print(entry['id'][:9])\n",
    "            node = TextNode(\n",
    "                    text=content_str,\n",
    "                    id_=id_e,  # 显式设置稳定ID\n",
    "                    metadata={\n",
    "                        \"股票代码\": entry['id'][:9],\n",
    "                    }\n",
    "                )\n",
    "            nodes.append(node)\n",
    "            continue\n",
    "    \n",
    "    print(f\"生成 {len(nodes)} 个文本节点（ID示例：{nodes[0].id_}）\")\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc33bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = create_nodes(json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d40593",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34674fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    EMBED_MODEL_PATH = r\"/Users/vitol/vscode/firm_perf/firm_perf_code/embed\"\n",
    "    LLM_MODEL_PATH = r\"/Users/vitol/vscode/firm_perf/firm_perf_code/model\"\n",
    "    \n",
    "    DATA_DIR = \"./data\"\n",
    "    VECTOR_DB_DIR = \"./chroma_db\"\n",
    "    PERSIST_DIR = \"./storage\"\n",
    "    \n",
    "    COLLECTION_NAME = \"firm_info\"\n",
    "    TOP_K = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_models():\n",
    "    \"\"\"初始化模型并验证\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(Config.LLM_MODEL_PATH, local_files_only=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(Config.LLM_MODEL_PATH, local_files_only=True)\n",
    "    # Embedding模型\n",
    "    embed_model = HuggingFaceEmbedding(\n",
    "        model_name=Config.EMBED_MODEL_PATH,\n",
    "        device=device,\n",
    "    )\n",
    "    \n",
    "    # LLM\n",
    "    llm = HuggingFaceLLM(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        model_kwargs={\n",
    "            \"trust_remote_code\": True,\n",
    "            \"device_map\": device\n",
    "        },\n",
    "        tokenizer_kwargs={\"trust_remote_code\": True},\n",
    "        generate_kwargs={\"temperature\": 0.3}\n",
    "    )\n",
    "    \n",
    "    Settings.embed_model = embed_model\n",
    "    Settings.llm = llm\n",
    "    \n",
    "    # 验证模型\n",
    "    test_embedding = embed_model.get_text_embedding(\"测试文本\")\n",
    "    print(f\"Embedding维度验证：{len(test_embedding)}\")\n",
    "    \n",
    "    return embed_model, llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f6550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vector_store(nodes: List[TextNode]) -> VectorStoreIndex:\n",
    "    chroma_client = chromadb.PersistentClient(path=Config.VECTOR_DB_DIR)\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\n",
    "        name=Config.COLLECTION_NAME,\n",
    "        metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "\n",
    "    # 确保存储上下文正确初始化\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    )\n",
    "\n",
    "    # 判断是否需要新建索引\n",
    "    if chroma_collection.count() == 0 and nodes is not None:\n",
    "        print(f\"创建新索引（{len(nodes)}个节点）...\")\n",
    "        \n",
    "        # 显式将节点添加到存储上下文\n",
    "        storage_context.docstore.add_documents(nodes)  \n",
    "        \n",
    "        index = VectorStoreIndex(\n",
    "            nodes,\n",
    "            storage_context=storage_context,\n",
    "            show_progress=True\n",
    "        )\n",
    "        # 双重持久化保障\n",
    "        storage_context.persist(persist_dir=Config.PERSIST_DIR)\n",
    "        index.storage_context.persist(persist_dir=Config.PERSIST_DIR)  # <-- 新增\n",
    "    else:\n",
    "        print(\"加载已有索引...\")\n",
    "        storage_context = StorageContext.from_defaults(\n",
    "            persist_dir=Config.PERSIST_DIR,\n",
    "            vector_store=ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "        )\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            storage_context.vector_store,\n",
    "            storage_context=storage_context,\n",
    "            embed_model=Settings.embed_model\n",
    "        )\n",
    "        # 安全验证\n",
    "    print(\"\\n存储验证结果：\")\n",
    "    doc_count = len(storage_context.docstore.docs)\n",
    "    print(f\"DocStore记录数：{doc_count}\")\n",
    "    \n",
    "    if doc_count > 0:\n",
    "        sample_key = next(iter(storage_context.docstore.docs.keys()))\n",
    "        print(f\"示例节点ID：{sample_key}\")\n",
    "    else:\n",
    "        print(\"警告：文档存储为空，请检查节点添加逻辑！\")\n",
    "    \n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e113f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model,llm = init_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3b2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(nodes):\n",
    "    QA_TEMPLATE = (\n",
    "    \"<|im_start|>system\\n\"\n",
    "    \"你是一个专业的上市公司研究员，你的知识库中有一系列的块级数据，每一个块是一个上市公司的基本信息、包括股价、年化指标、员工评论数据。在检索公司信息时，首先，你需要根据客户提供的公司名和每个块的'id_'进行匹配，找到唯一的、最相似的块，然后就仅仅基于这个最相似的块进行进一步检索，不需要再检索其他块的信息。在检索到块后，你需要严格根据块内提供的公司信息，用中文回答问题。必要时根据客户的要求针对公司进行基于信息的适当分析。如果实在索引不到知识库中所提供的公司信息数据，那么请回复：我不知道。如果客户的问题比较泛化，比如问：“你对某个公司的发展有什么看法？”，或者问：“评价一下某公司？”，那么请根据公司信息进行分析总结后给出你的看法。最重要的一点是！你需要尽可能快的给我回复！\\n\"\n",
    "    \"相关的上市公司信息数据：\\n{context_str}\\n<|im_end|>\\n\"\n",
    "    \"<|im_start|>user\\n{query_str}<|im_end|>\\n\"\n",
    "    \"<|im_start|>assistant\\n\"\n",
    ")\n",
    "    response_template = PromptTemplate(QA_TEMPLATE)\n",
    "    \n",
    "    \n",
    "    # # 仅当需要更新数据时执行\n",
    "    # if not Path(Config.VECTOR_DB_DIR).exists():\n",
    "    #     print(\"\\n初始化数据...\")\n",
    "\n",
    "    # else:\n",
    "    #     nodes = None  # 已有数据时不加载\n",
    "    \n",
    "    print(\"\\n初始化向量存储...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    index = init_vector_store(nodes)\n",
    "    print(f\"索引加载耗时：{time.time()-start_time:.2f}s\")\n",
    "    \n",
    "    # 创建查询引擎\n",
    "    query_engine = index.as_query_engine(\n",
    "        similarity_top_k=Config.TOP_K,\n",
    "        # text_qa_template=PromptTemplate(text_qa_template),\n",
    "        # text_qa_template=response_template,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # 示例查询\n",
    "    while True:\n",
    "        question = input(\"\\n请输入公司相关问题（输入q退出）: \")\n",
    "        if question.lower() == 'q':\n",
    "            break\n",
    "        \n",
    "        # 执行查询\n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        # 显示结果\n",
    "        print(f\"\\n智能助手回答：\\n{response.response}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf36528",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(nodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firm_judgement",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
