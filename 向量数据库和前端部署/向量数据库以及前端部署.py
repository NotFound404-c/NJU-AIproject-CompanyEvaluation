import streamlit as st
from llama_index.llms.openai import OpenAI
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.vector_stores.milvus import MilvusVectorStore
from pymilvus import MilvusClient
import asyncio
from llama_index.llms.openai import OpenAI
from llama_index.core.schema import TextNode
import time
from pathlib import Path
from typing import List, Dict
import os
import chromadb
from llama_index.core import VectorStoreIndex, StorageContext, Settings
from llama_index.core.schema import TextNode
from llama_index.llms.huggingface import HuggingFaceLLM
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import PromptTemplate
from transformers import AutoModelForCausalLM, AutoTokenizer
from llama_index.core.llms import ChatMessage
from llama_index.llms.huggingface import HuggingFaceLLM
import torch
import pandas as pd 
import json
from llama_index.vector_stores.milvus import MilvusVectorStore
from llama_index.core import StorageContext, VectorStoreIndex
from llama_index.embeddings.openai import OpenAIEmbedding
from pymilvus import model

# Streamlit ç•Œé¢
st.set_page_config(page_title="Company-Evaluation", page_icon="ğŸ¦œğŸ”—")
st.title("Company-Evaluation")

# åˆå§‹åŒ–ç»„ä»¶
@st.cache_resource
def init_query_engine():
    def create_vector_store():
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        return MilvusVectorStore(
            uri="/root/milvus_filter_demo.db",
            collection_name="companyinfo",
            dim=512,
            overwrite=False,
        )

    vector_store = create_vector_store()
    # åˆ›å»º LlamaIndex çš„ OpenAI LLM å®ä¾‹
    embed_model = HuggingFaceEmbedding(
        model_name=r"/root/embed"
    )
    
    llm = OpenAI(
        model="gpt-4o",
        api_base='your-apibase',
        api_key='your-apikey'
    )

    # client = MilvusClient(uri="milvus_filter_demo.db")
    # è¿æ¥åˆ°æœ¬åœ° Milvus Liteï¼ˆsqlite æ–‡ä»¶ï¼‰
    # æ³¨æ„ï¼šuri éœ€è¦ç”¨ sqlite:/// å‰ç¼€ï¼Œä¸”è·¯å¾„è¦ç»å¯¹è·¯å¾„
    #vector_store = MilvusVectorStore(
     #   uri="./milvus_filter_demo.db",
      #  collection_name="companyinfo",
       # dim=512,
        #overwrite=True,
   # )

    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_vector_store(vector_store,embed_model=embed_model)

    # åˆ›å»ºæŸ¥è¯¢å¼•æ“
    query_engine = index.as_query_engine(
        llm=llm,
        similarity_top_k=1
    )

    return query_engine


# æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–æ¨¡å‹
if 'query_engine' not in st.session_state:
    st.session_state['query_engine'] = init_query_engine()

def get_response_text(question):
    response = st.session_state['query_engine'].query(question)
    # æå–çº¯æ–‡æœ¬å“åº”ï¼Œç§»é™¤ä»»ä½•é¢å¤–èŠ‚ç‚¹ä¿¡æ¯
    return str(response).split("\n\n")[0]

if "messages" not in st.session_state.keys():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯AIå°å—ï¼Œæˆ‘å¯ä»¥è¯„ä»·å¾ˆå¤šAè‚¡ä¸Šå¸‚å…¬å¸!"}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

def clear_chat_history():
    st.session_state.messages = [{"role": "assistant", "content": "ä½ å¥½ï¼Œæˆ‘æ˜¯AIå°å—ï¼Œæˆ‘å¯ä»¥è¯„ä»·å¾ˆå¤šAè‚¡ä¸Šå¸‚å…¬å¸!"}]

st.sidebar.button('Clear Chat History', on_click=clear_chat_history)

def generate_response(prompt_input):
    return get_response_text(prompt_input)

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            # è·å–çº¯æ–‡æœ¬å“åº”
            response_text = generate_response(prompt)
            
            # ç¡®ä¿åªæ˜¾ç¤ºçº¯æ–‡æœ¬å†…å®¹
            if "Response object" in response_text:
                response_text = response_text.split("Response object.")[0]
            
            # ç§»é™¤å¯èƒ½çš„é¢å¤–èŠ‚ç‚¹ä¿¡æ¯
            if "source_nodes" in response_text:
                response_text = response_text.split("source_nodes")[0]
            
            # æ˜¾ç¤ºå“åº”
            st.write(response_text)
    
    # å­˜å‚¨çº¯æ–‡æœ¬å“åº”
    message = {"role": "assistant", "content": response_text}
    st.session_state.messages.append(message)
# è¿è¡Œå‘½ä»¤ï¼šstreamlit run /root/finc.py
